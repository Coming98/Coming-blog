---
title: 感知机模型
mathjax: true
date: 2021-07-09 22:45:14
summary: 统计学习方法第一弹~
categories: 统计学习方法
tags: ML
---

![image-20210709204458015](https://gitee.com/Butterflier/pictures/raw/master/image-20210709204458015.png)

本文来自于书籍以及自己的一些想法，内容不一定正确，还请大家批评指正。

# 定义

* 初识：二类分类的简单线性分类模型，属于判别模型

* 输入：$\mathcal{X} \subseteq \mathbf{R}^n$ 

  > $\mathcal{X}$ 为样本空间或状态空间，$\mathbf{R}^n$ 为 $n$ 维向量空间（实数空间）
  >
  > $\mathcal{X} = \{x_1, x_2, \dots, x_n\}$

* 输出：$\mathcal{Y} \in \left\{ +1, -1 \right\}$

  > 输出为实例的类别

* 目标：**假设数据集是线性可分的**，寻找一个超平面，将训练数据全部分类正确

  > ![image-20210709210455875](https://gitee.com/Butterflier/pictures/raw/master/image-20210709210455875.png)

* 扩展：感知机可以称为单层的神经网络，而多层的感知机则称为神经网络；是神经网络与支持向量机的基础。

# 模型

$$
f(x) = sign(w \cdot x + b)
$$

模型的产生来源于目标：寻找一个超平面，将训练数据全部分类正确，针对二维线性可分数据，我们可以寻找到 $Ax+By+C=0$ 这样一个平面，将数据集完全分开。而 $Ax+By+C=0$  不就是 $w_1x_1+w_2x_2+b=0$ 即 $wx+b$ 吗，因此我们成功将二维问题扩展开来。

* $w$ 为权值向量，为每一维度特征向量分配权重 (weight)
* $b$ 为偏置，提供一个平移变换

现在我们能够依靠 $wx+b > 0$ 或  $wx+b < 0$ 将数据集简单分割开来，为了实现分类效果，引入了符号函数 $\mathop{sign}$
$$
sign(x)=
\begin{cases}
1& \text{x>0}\\
0& \text{x<=0}
\end{cases}
$$
到此，我们便构造出了以 “**假设数据集是线性可分的**，寻找一个超平面，将训练数据全部分类正确” 这一目的为导向的假设空间，也就是说，针对这样一类我们，感知机模型认为，解决这类问题的最优函数一定存在与假设空间之中，也就是函数集合 $\{f | f(x) = w \cdot x + b\}$

# 策略

再次回顾 前提 与 目标。

* 前提：数据集具有线性可分性
* 目标：求得一个能将训练集正实例点和负实例点完全正确分开的超平面

那么应该怎么评判我们选择的模型的好坏呢？

* 初始的感觉是：模型误分类点的个数，但是这样的函数离散，并非参数 $w,b$ 的连续可到函数，不易用于选择参数 $w,b$ ，或者说我们如何评判分类错误的点对参数 $w, b$ 更新的指导程度？假定 $M$ 为误分类点的集合，那我们的 $loss = \sum\limits_{x_i\in M} 1$，很显然这样的策略或者损失函数，对参数 $w, b$ 更新没有指导意义？
* 后续思考，那我应该将参数 $w, b$ 用于计算 $loss$，所以可能会想到使用误分类点到平面的距离作为 loss function = $\dfrac{1}{||w||}|w \cdot x_0 + b|$ ，进一步优化一下去掉绝对值 = $-\dfrac{y_i}{||w||}|w \cdot x_0 + b|$ (注意这是针对误分类点来说的嗷)
* 继续优化 $-\dfrac{1}{||w||}$ 对于后续的梯度下降求导来说较为复杂，针对 $x_i$ 的函数，$w, b$ 在进行 loss 计算时充当常数，因此可否将 $-\dfrac{1}{||w||}$ 舍去呢？答案是肯定的
  1. $-\dfrac{1}{||w||}$ 在 loss function 中充当系数，消除后仅仅缩放的结果域，将当于 $3x+6y+9=0$ 变为 $x+2y+3=0$，但是每个误分类点对参数 $w, b$ 更新的指导程度确实是降低了。
  2. 而感知机学习算法是误分类驱动的, 目标是分类是否完全正确。因此，存在一个分类不正确的点都是不允许的，不用看那个点产生多少 Loss 值。
* 最终确定了损失函数：$L(w, b) = -\sum\limits_{x_i\in M}y_i(w \cdot x_i + b)$

## 点到超平面的距离

求点 $x_0$ 到超平面 $S$：$w·x+b=0$ 的距离

1. 设点 $x_0$ 在平面 $S$ 上的投影为 $x_1$，则$w\cdot x_1+b=0$

2. 向量 $\overrightarrow{x_0x_1}$ 与 $S$ 的法向量平行，夹角为 0
3. 通过 1 2 构造等式

$$
|w \cdot \overrightarrow{x_0x_1} | = |w|\times|\overrightarrow{x_0x_1}|cos(0) = ||w|| \times d
$$

4. 另一方面

$$
w \cdot \overrightarrow{x_0x_1} = w^1(x_0^1-x_1^1)+\dots+w^n(x_0^n-x_1^n)
= w \cdot x_0 - w \cdot x_1
$$

由第 1 点可得 $w \cdot x_1 = -b$ , 因此：
$$
w \cdot \overrightarrow{x_0x_1} = w \cdot x_0 + b
$$

5. 所以距离 $d$ 也就因此得出

$$
d = \dfrac{1}{||w||}|w \cdot x_0 + b|
$$

# 算法

采用我们熟知的梯度下降法，计算梯度，更新参数。

目标最小化 loss 直至 0，因此需要向梯度的负方向更新参数

**梯度**：
$$
\bigtriangledown_{w}L(w,b)=-\sum\limits_{x_i \in M} y_ix_i \\
\bigtriangledown_{b}L(w,b)=-\sum\limits_{x_i \in M} y_i
$$
**针对误分类点 $\mathbf{x_i}$ 执行更新** 
$$
w \leftarrow w + \eta y_i x_i \\
b \leftarrow b + \eta y_i
$$


# 感知机原始形式

我们的策略是经验风险最小化：
$$
\mathop{min}\limits_{w,b}L(w,b) = -\sum\limits_{x_i\in M}y_i(w \cdot x_i + b)
$$
算法的基本流程图如下所示：

1. 初始化 $w$ 和 $b$ 的值

2. 在训练数据集中选取数据 $(xi, yi)$

3. 如果 $y_i(w\cdot x_i + b) \le 0$ 

   T：表示该数据误分类，采用梯度下降法进行更新
   F：进行步骤 4

4. 跳转至步骤2，直至训练集中没有误分类点

<img src="https://gitee.com/Butterflier/pictures/raw/master/image-20210709220511277.png" alt="image-20210709220511277" style="zoom:50%;" />

## 函数实现

### 预准备

首先可以定义一个函数，感知机模型最终就要找到这个函数的表示形式

```python
def Fun(x_1, x_2):
    # w = (3, -4)  b = 5
    y = 3 * x_1 - 4 * x_2 + 5
```

追加符号函数

```python
def sign(x):
    # 代入点进入如果值为0表示在线上，如果值 > 0表示在线的一侧，小于 0 在另一侧
    return 1 if Fun(*x) >= 0 else -1
```

生成数据

```python
size = (500, 2)
# (, 2) # 坐标面生成随机点
X = np.random.randint(low = -11, high = 10, size = size) 
y = np.array([sign(x) for x in X]) # 按照预设线条分点
```

可视化展示期望的最终效果

```python
def showPerceptron(X, y, w, b):
    print('w = ', w)
    print('b = ', b)
    plt.scatter(X[:, 0][y > 0], X[:, 1][y > 0], color = 'r', s=3.)
    plt.scatter(X[:, 0][y < 0], X[:, 1][y < 0], color = 'b', s=3.)
    x_axis = np.linspace(np.min(X)-1, np.max(X)+1, 300)
    # 一般式转化为斜截式
    y_axis = [-w[0]/w[1] * x - b / w[1] for x in x_axis]
    plt.plot(x_axis, y_axis)
    plt.show()
```

![image-20210709221404570](https://gitee.com/Butterflier/pictures/raw/master/image-20210709221404570.png)

### 实现

```python
class Perceptron_1:
    def __init__(self, learning_rate = 1.0):
        self.w = 0.
        self.b = 0.
        self.learning_rate = 1.
    
    def fun(self, x):
        # 一个样本
        return np.dot(x, self.w) + self.b
    
    def train(self, X, y):
        self.w = np.zeros(len(X[0])) # 真正初始化权重值
        flag = True # 表示继续学习
        while flag:
            flag = False # 猜测本次能够学习完毕
            for i in range(len(X)):
                x_i = X[i] # (2, )
                y_i = y[i] # (1, )
                if(y_i * self.fun(x_i) <= 0.):
                    self.w += self.learning_rate * y_i * x_i
                    self.b += self.learning_rate * y_i
                    flag = True # 猜测失误
        # 展示结果
        showPerceptron(X, y, self.w, self.b)
```

**效果展示如下**

```python
my_perceptron = Perceptron_1()
my_perceptron.train(X, y)
```

![image-20210709221558656](https://gitee.com/Butterflier/pictures/raw/master/image-20210709221558656.png)

# 感知机对偶形式

（确实还未能理解换形式表达的真正原因，仅仅记录些书面看到的信息啦~）

* 创新点：将 $w$ 和 $b$ 表示为实例 $x_i$ 和 $y_i$ 的线性组合的形式，通过求解其系数而求得 $w$ 和 $b$

例如 $w$ 和 $b$ 初始值为 0，在第本次更新时的取值如下：
$$
\begin{align}
w &= \sum_{i=1}^{N} \alpha_i y_i x_i \\
b &= \sum_{i=1}^{N} \alpha_i y_i \\
\alpha_i &= n_i \eta_i
\end{align}
$$
在这要注意 $N$ 表示实例总数，我们更新的总次数未知，只知道截止目前各个样本点用于更新的次数，因此遍历样本，通过当前样本 $x_i$ 对参数的更新贡献次数，计算参数的值

算法的基本流程图如下所示：

1. 初始化 alpha 和 b 的值

2. 在训练数据集中选取数据 $(x_i, y_i)$

3. 如果 $y_i(\sum_{i=1}^{N} \alpha_i y_i x_i\cdot x_i + b) \le 0$ 

   T：表示该数据误分类，采用梯度下降法进行更新
   F：进行步骤 4

4. 跳转至步骤2，直至训练集中没有误分类点

<img src="https://gitee.com/Butterflier/pictures/raw/master/image-20210709223032341.png" alt="image-20210709223032341" style="zoom:50%;" />

### 函数实现

```python
class Perceptron_2:
    def __init__(self, learning_rate=1.):
        self.alpha = 0.
        self.b = 0.
        self.learning_rate = 1.
        self.gram = 0.
    def get_w_b(self, X, y):
        """ 
            采用矩阵计算的形式提高速度，并且简化运算，不必封装
            length = len(self.alpha)
            w, b = 0, 0
            for i in range(length):
                w += self.alpha[i] * y[i] * X[i]
                b += self.alpha[i] * y[i]
        """
        w = np.dot((self.alpha * y).T, X)
        b = np.dot(self.alpha, y.T)
        return (w, b)
    def fun(self, X, y, aim_index):
        # return np.dot(w, X[aim_index]) + b
        # alpha, y, x转为矩阵运算的形式：alpha与y直接内积运算，然后与gram矩阵对应相乘并相加（矩阵乘法）
        return np.dot((self.alpha * y).T, self.gram[aim_index]) + np.dot(self.alpha, y.T)
    
    def train(self, X, y):
        self.alpha = np.zeros(len(X)) # 表明每个节点被误分类的次数 * learning-rate
        self.b = 0.
        self.gram = np.dot(X, X.T) # 快速的运算
        flag = True
        while flag:
            flag = False
            for i, data in enumerate(X):
                if(y[i] * self.fun(X, y, i) <= 0):
                    self.alpha[i] += self.learning_rate
                    self.b += self.learning_rate * y[i]
                    flag = True
        w, b = self.get_w_b(X, y)
        showPerceptron(X, y, w, b)
```

**效果展示**

```python
my_perceptron = Perceptron_2()
my_perceptron.train(X, y)
```

![image-20210709223249283](https://gitee.com/Butterflier/pictures/raw/master/image-20210709223249283.png)

书上 P45 页例子的展示

```python
X_1 = np.array([
    [3, 3],
    [4, 3],
    [1, 1]
])
y_1 = np.array([1, 1, -1])
my_perceptron = Perceptron_2()
my_perceptron.train(X_1, y_1)
```

![image-20210709223338511](https://gitee.com/Butterflier/pictures/raw/master/image-20210709223338511.png)

---

Thanks !
